% Chapter Template

\chapter{Methods} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 3. \emph{Methods}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION: INTRO
%----------------------------------------------------------------------------------------

\section{Introduction}
In this chapter we describe various common uncertainty quantification methods and their applications. We begin
by discussing the principles of input spaces and responses, and define terminology used in this work.  Next we
discuss uncertainty quantification at a high level, and describe several common uncertainty quantification
tools.  Finally, we explore generalized polynomial chaos expansion, stochastic collocation, and high-density
model reduction as advanced uncertainty quantification techniques.

% inputs and outputs
Many simulation models are algorithms constructed to solve partial differential equations, often in two or
three dimensions and possibly time.  The inputs to these models include boundary conditions, material
properties, tuning parameters, and so forth.  The outputs are quantities of interest, either data fields or
scalar values.  The outputs are used to inform decision-making processes.  In general, we allow $u(Y)$ to be
the model $u$ as a function of the input space $Y = (y_1,\ldots,y_n,\ldots,y_N)$ where $y_n$ is a single input
parameter to the model, $n$ is an index spanning the number of inputs, and $N$ is the total number of inputs.
We signify the output response quantity as $Q$, and assume it to be a scalar integrated quantity.  Our generic 
model takes the form
\begin{equation}
  u(Y) = Q.
\end{equation}

% uncertain inputs
Essential to using simulation models is understanding the possibility of important uncertainties existing in
the inputs.  These could be aleatoric uncertainties due to intrinsic randomness in the inputs, or epistemic
uncertainties due to model imperfections or lack of knowledge.  Each of these uncertainties has some
distribution defining the likelihood of an input to have a particular value.  These distributions might be
assumed or constructed from experiment; for our work, we will assume given distributions are accurate.  The
input likelihood distribution is the probability distribution function (PDF) $\rho_n(y_n)$.  We require
\begin{equation}
  \int_a^b \rho_n(y_n) d\ y_n = 1,
\end{equation}
where $a$ and $b$ are the minimum and maximum values $y_n$ can take (possibly infinite).

% multidimensional
When there are more than one uncertain input, the combination of distributions for these inputs span an
uncertainty space $\Omega$. TODO make sure this terminology is right.  The dimensionality of $\Omega$ is $N$,
the number of uncertain input variables.  The probability of any point in the input space occurring is given
by the join-probability distribution $\rho(Y)$, still enforcing
\begin{equation} \label{eq:joint pdf}
  \int_{a_1}^{b_1}\cdots\int_{a_N}^{b_N} \rho(Y) dy_1\cdots dy_N = 1.
\end{equation}.
For clarity, we define multidimensional integral operator
\begin{equation}
  \int_\Omega (\cdot)dY\equiv \int_{a_1}^{b_1}\cdots\int_{a_N}^{b_N} (\cdot) dy_1\cdots dy_N,
\end{equation}
so that Eq. \ref{eq:joint pdf} can be written
\begin{equation}
  \int_\Omega \rho(Y) dY = 1.
\end{equation}

% correlation
We note the possibility that multiple inputs may be correlated with each other.  When inputs are not
independent, the joint probability distribution is not the product of each individual probability distribution
distribution.  Using principle component analysis (sometimes known as Karhunen-Loeve expansion
\cite{karhunen}), however, a surrogate orthogonal input space can be
constructed.  As a result, we only consider independent variables in this work.



\section{Uncertainty Quantification}
% introduction
The purpose of uncertainty quantification is to propagate the uncertainties present in the input space of a
problem through the model and comprehend their effects on the output responses.  Often response uncertainty is
quantified in terms of moments.  Second-order uncertainty quantification seeks for the mean and variance of
the perturbed response.  In general, the mean of a model is the first moment,
\begin{equation}
  \text{mean} = \expv{u(Y)} = \int_\Omega \rho(Y) u(Y) dY,
\end{equation}
and the variance is the second moment less the square of the first,
\begin{equation}
  \text{variance} = \expv{u(Y)^2} - \expv{u(Y)}^2 = \int_\Omega \rho(Y) u(Y)^2 dY - \text{mean}^2.
\end{equation}

%sensitivity analysis
Another use for uncertainty quantification is understanding the sensitivity of the output responses to the
uncertain inputs; that is, determining how responses change as a function of changes in the input space.  At
the most primitive level, linear sensitivity of a response mean to an input is the derivative of the response
with respect to the input.  Sensitivities can be both local to a region in the input space as well as global
to the entire problem.

There are several common tools used for uncertainty quantification when analytic analysis is not possible.
These include stochastic methods such as Monte Carlo sampling, deterministic methods such as Grid sampling,
and mixed methods such as Latin Hypercube sampling (LHS).

\subsection{Monte Carlo}
The Monte Carlo method \cite{mc} has been used formally since the 1930s as a tool to explore possible outcomes
in uncertain models.  Nuclear physicist Enrico Fermi used the method in his work with neutron moderation in
Rome \cite{mcfermi}.  In its simplest form, Monte Carlo involves randomly picking realizations from a set of
possibilities, then statistically collecting the results.  In uncertainty quantification, Monte Carlo can be
used to sample points in the input space based on the joint probability distribution.  The collection of
points is analyzed to determine the moments of the response.

The mean of a response is determined using the unweighted average of samples collected:
\begin{equation}
  \expv{u(Y)} = \frac{1}{N}\sum_{m=1}^M \qty(u(Y_m)) + \eps_M^{\text{MC}},
\end{equation}
where $Y_m$ is a realization randomly chosen based on $rho(Y)$, and $M$ is the total number of samples taken.
The error in the approximation diminishes with the root of the number of samples taken,
\begin{equation}
  \eps_M^{\text{MC}} \propto \frac{1}{\sqrt{M}}.
\end{equation}
The second moment is similarly approximated as
\begin{equation}
  \expv{u(Y)^2} \approx \frac{1}{N}\sum_{m=1}^M \qty(u(Y_m)^2).
\end{equation}
The standard deviation (root of the variance) converges similarly to the mean for Monte Carlo methods.  There
are many tools that can be used to improve Monte Carlo sampling \cite{mcvarred}\cite{mcnpvarred}; we restrict
our discussion to traditional analog Monte Carlo sampling.

Monte Carlo has long been a gold standard for uncertainty quantification because of its consistency.  Monte
Carlo will always resolve the response statistics given a sufficient number of samples.  Additionally, the
convergence of Monte Carlo is largely agnostic of the input space dimensionality, a feature not shared by the
LHS and Grid sampling methods.

The drawback to Monte Carlo sampling also centers on its consistency.  The error in analog Monte Carlo can only be
consistently reduced by drastically increasing the number of samples calculated.  While coarse estimates are
inexpensive to obtain, high precision takes a great deal of runs to converge.

