% Chapter Template

\chapter{Uncertainty Quantification Techniques} % Main chapter title

\label{ch:methods} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 3. \emph{UQ Techniques}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Uncertainty Quantification Methods}
We consider a few methods for uncertainty quantification (UQ).  One method to classify UQ methods is by their
interaction level with a simulation code.  Non-intrusive methods treat the code as a black box, perturbing the
inputs and collecting the outputs without modifying the code.  These methods are ideal for generic application
frameworks, where the simulation code may be unknown or precompiled.  Examples of non-intrusive methods
include analog Monte Carlo (MC), Latin Hypercube sampling (LHS), and deterministic collocation methods.
Alternatively, intrusive methods require access to the solution algorithm itself.  Sometimes this can provide
more efficient solutions.  In particular, adjoint methods require the solution operator to be reversible, and
provide very efficient methods to determine sensitivities and analyze output-input dependence.  While many
intrusive methods have benefits, they lack the flexibility and universal applicability of non-intrusive
methods, and so we neglect them in this work.

\section{Correlation}
An assumption we make going forward is that the uncertain input parameters are independent or at least
uncorrelated.  If this is not the case, methods such as Karhunen-Loeve expansion can be used to develop an orthogonal
input space to replace the original.  Similarly, principle component analysis can be used to attempt to find
the fundamental uncorrelated space that maps into the correlated variable space.

\section{Monte Carlo}
In analog Monte Carlo uncertainty quantification, a single point in the input space is selected randomly,
and an output collected, until an appropriately accurate view of the output space is acquired.  While few
samples result in a poor understanding of the quantity of interest, sufficiently large samples converge on a
correct solution.  The convergence rate of Monte Carlo is consistent, as
\begin{equation}
  \epsilon = \frac{c}{\sqrt{\eta}},
\end{equation}
where $c$ is a constant and $\eta$ is the number of samples taken.  While this convergence rate is slow, it is possibly one of the
most reliable methods available.  This makes MC a good choice for benchmarking.

One of the downsides of MC (and LHS) when compared with other methods is that they do not generate a
reduced-order model as part of the evaluation; however, interpolation methods can be used to generate
additional samples.

\section{Latin Hypercube Sampling}
Latin hypercube sampling\cite{lhs} is another stochastic method that specializes in multidimensional
distributions.  In this method, the input domain is divided into $M^N$ subdomains that contain an equal
probability volume, with $M$ divisions along each axis.  Sampling is then performed in these volumes, assuring
no two samples share the same ``row'' for a given input dimension; that is, once a sample is taken within the
$i$-th subdivision of an input range (axis), another sample may not be taken that is within that subdivision
for that variable.  Thus, subdividing each axis into $M$ sections results in $M$ total possible samples.

LHS is useful as a sampling strategy because it assures samples are distributed throughout the entirety of the
input space, which is less guaranteed using MC sampling.  There is some cost, however, associated with
subdividing and tracking the grid on the input space.



\section{Generalized Polynomial Chaos}
In general, polynomial chaos expansion (PCE) methods seek to represent the simulation code as a combination of
polynomials of varying degree and combination in each dimension of the input space.  Originally Wiener
proposed expanding in Hermite polynomials for Gaussian-normal distributed variables \cite{wiener}.  Askey and
Wilson generalized this method for a range of Gaussian-based distributions with corresponding polynomials,
including Legendre polynomials for uniform distributions, Laguerre polynomials for Gamma distributions, and
Jacobi polynomials for Beta distributions\cite{Wiener-Askey}.

In each of these cases, a probability-weighted
integral over the distribution can be cast in a way that the corresponding polynomials are orthogonal over the
same weight and interval.  These chaos Wiener-Askey polynomials were used by Xiu and Karniadakis to develop
the generalized polynomial chaos expansion method (gPC), including a transformation for applying the same
method to arbitrary distributions (as long as they have a known inverse CDF)\cite{xiu}.  Two significant
methodologies have grown from gPC application.  The first makes use of Lagrange polynomials to expand the
original function or simulation code, as they can be made orthogonal over the same domain as the
distributions\cite{SCLagrange}; the other uses the Wiener-Askey polynomials\cite{xiu}.  We consider the latter in this work.

We consider a simulation code that produces a quantity of interesy $u$ as a function $u(Y)$ whose arguments are the uncertain, distributed input
parameters $Y=[Y_1,\ldots,Y_n,\ldots,Y_N]$.  A particular realization $\omega$ of $Y_n$ is expressed by
$Y_n(\omega)$, and a single realization of the entire input space results in a solution to the function as
$u(Y(\omega))$.  We acknowledge obtaining a realization of $u(Y)$ may take considerable computation time and
effort, and may be solved nonlinearly and without analytic solution.  There may be other input parameters that
contribute to the solution of $u(Y)$; we neglect these, as our interest is chiefly in the uncertainty space.
In addition, it is possible that the quantity of interest $u(Y)$ is an integrated quantity or some norm of a
value that is temporally or spatially distributed; in any case, we restrict $u(Y(\omega))$ to a single scalar
output.

We expand $u(Y)$ in orthonormal multidimensional polynomials $\Phi_k(Y)$, where $k$ is a multi-index tracking
the polynomial in each axis of the polynomial Hilbert space, and $\Phi_k(Y)$ is constructed as
\begin{equation}\label{eq:gPC}
  \Phi_k(Y) = \prod_{n=1}^N \phi_{k_n}(Y_n),
\end{equation}
where $\phi_{k_n}(Y_n)$ is a single-dimension Wiener-Askey orthonormal polynomial of order $k_n$ and
$k=[k_1,\ldots,k_n,\ldots,k_N]$.  The gPC for $u(Y)$ using this notation is
\begin{equation}
  u(Y) \approx \sum_{k\in\Lambda(L)} u_k\Phi_k(Y),
\end{equation}
where $u_k$ is a weighting polynomial coefficient. The polynomials used in the expansion are determined
by the set of multi-indices $\Lambda(L)$, where $L$ is a truncation order for isotropic methods.  In the limit
that $\Lambda$ contains all possible combinations of polynomials of any order, Eq. \ref{eq:gPC} is exact.
Practically, however, $\Lambda$ is truncated to some finite set of combinations, discussed in section
\ref{sec:index sets}.

Using the orthonormal properties of the Wiener-Askey polynomials,
\begin{equation}
  \int_\Omega \Phi_k(Y)\Phi_{\hat k}(Y) \rho(Y) dY = \delta_{k\hat k},
\end{equation}
where $\rho(Y)$ is the combined PDF of $Y$, $\Omega$ is the multidimensional domain of $Y$, and $\delta_{nm}$
is the Dirac delta, we can isolate an expression of the polynomial expansion coefficients.
We multiply both sides of Eq. \ref{eq:gPC} by
$\Phi_{\hat k}(Y)$, integrate both sides over the probability-weighted input domain, and sum over all $\hat k$
to obtain the coefficients, sometimes referred to as polynomial expansion moments,
\begin{align}\label{eq:polycoeff}
  u_k &= \frac{\langle u(Y)\Phi_k(Y) \rangle}{\langle \Phi_k(Y)^2 \rangle},\\
      &= \langle u(Y)\Phi_k(Y) \rangle,
\end{align}
where we use the angled bracket notation to denote the probability-weighted inner product,
\begin{equation}
  \langle f(Y) \rangle \equiv \int_\Omega f(Y)\rho(Y) dY.
\end{equation}
When $u(Y)$ has an analytic form, these coefficients can be solved by integration; however, in general other
methods must be applied to numerically perform the integral.  While tools such as Monte Carlo integration can
be used to evaluate the integral, we can harness the properties of Gaussian quadratures because of the
probability weights and domain.  This stochastic collocation method is discussed in section \ref{sec:stoch
coll}.


\subsection{Polynomial Index Set Construction}\label{sec:index sets}
There are many ways by which a polynomial index set can be constructed.  Here we present three: tensor
product, total degree, and hyperbolic cross.

In the nominal tensor
product case, $\Lambda(L)$ contains all possible combinations of polynomial indices up to truncation order $L$ in each
dimension, as
\begin{equation}
  \Lambda_\text{TP}(L)=\Big\{\bar p=[p_1,\cdots,p_N]: \max_{1\leq n\leq N}p_n\leq L
\Big\}.
\end{equation}
The cardinality of this index set is $|\Lambda_\text{TP}(L)|=(L+1)^N$. For example, for a two-dimensional
input space ($N$=2) and truncation limit $L=3$, the index set $\Lambda_\text{TP}(3)$ is given in Table
\ref{tab:TP}, where the notation $(1,2)$ signifies the product of a polynomial that is first order in $Y_1$
and second order in $Y_2$.

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c}
    (3,0) & (3,1) & (3,2) & (3,3) \\
    (2,0) & (2,1) & (2,2) & (2,3) \\
    (1,0) & (1,1) & (1,2) & (1,3) \\
    (0,0) & (0,1) & (0,2) & (0,3)
  \end{tabular}
  \caption{Tensor Product Index Set, $N=2,L=3$}
  \label{tab:TP}
\end{table}

It is evident there is some inefficiencies in this index set.  First, it suffers dramatically from the
\emph{curse of dimensionality}; that is, the number of polynomials required grows exponentially with
increasing dimension.  Second, the total order of polynomials is not considered.  Assuming the contribution of
each higher-order polynomial is smaller than lower-order polynomials as per gPC, the (3,3) term is
contributing sixth-order corrections that are likely smaller than the error introduced by ignoring
fourth-order corrections (4,0) and (0,4).  This leads to the development of the \emph{total degree} (TD) and
\emph{hyperbolic cross} (HC) index set construction strategies\cite{hctd}.

In TD, only multidimensional polynomials whose \emph{total} order is less than $L$ are permitted, as
\begin{equation}
  \Lambda_\text{TD}(L)=\Big\{\bar p=[p_1,\cdots,p_N]:\sum_{n=1}^N p_n \leq L
\Big\}.
\end{equation}
The cardinality of this index set is $|\Lambda_\text{TD}(L)|={L+N\choose N}$, which grows with increasing
dimension much more slowly than TP.  For the same $N=2,L=3$ case above, the TD index set is given in Table
\ref{tab:TD}. 

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c}
    (3,0) &       &       &       \\
    (2,0) & (2,1) &       &       \\
    (1,0) & (1,1) & (1,2) &       \\
    (0,0) & (0,1) & (0,2) & (0,3)
  \end{tabular}
  \caption{Total Degree Index Set, $N=2,L=3$}
  \label{tab:TD}
\end{table}

In HC, the \emph{product} of polynomial orders is used to restrict allowed polynomials in the index set.  This
tends to polarize the expansion, emphasizing higher-order polynomials in each dimension but lower-order
polynomials in combinations of dimensions, as
\begin{equation}
  \Lambda_\text{HC}(L)=\Big\{\bar p=[p_1,\ldots,p_N]:\prod_{n=1}^N p_n+1 \leq L+1
\Big\}.
\end{equation}
The cardinality of this index set is bounded by $|\Lambda_\text{HC}(L)|\leq (L+1)(1+\log(L+1))^{N-1}$. It
grows even more slowly than TD with increasing dimension, as shown in Table \ref{tab:HC} for $N=2,L=3$.

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c}
    (3,0) &       &       &       \\
    (2,0) &       &       &       \\
    (1,0) & (1,1) &       &       \\
    (0,0) & (0,1) & (0,2) & (0,3)
  \end{tabular}
  \caption{Hyperbolic Cross Index Set, $N=2,L=3$}
  \label{tab:TD}
\end{table}

It has been shown that the effectiveness of TD and HC as index set choices depends strongly on the regularity
of the output space\cite{hctd}.  TD tends to be most effective for infinitely-continuous response surfaces,
while HC is more effective for surfaces with limited smoothness or discontinuities.

\subsection{Anisotropy}
While using TD or HC to construct the polynomial index set combats the curse of dimensionality present in TP,
it is not eliminated and continues to be a problem for problems of large dimension.  Another method that can
be applied to mitigate dimensionality problem is anisotropy, or the unequal treatment of multiple dimensions.
In this strategy, weighting factors $\alpha=[\alpha_1,\ldots,\alpha_n,\ldots,\alpha_N]$ are applied in each
dimension to allow additional polynomials in some dimensions and less in others.  This change adjusts the TD
and HC construction rules as follows, where $|\alpha|_1$ is the one-norm of $\alpha$.
\begin{equation}
  \tilde\Lambda_\text{TD}(L)=\Big\{\bar p=[p_1,\cdots,p_N]:\sum_{n=1}^N \alpha_n p_n \leq \qty|\vec\alpha|_1 L
\Big\},
\end{equation}
\begin{equation}
  \tilde\Lambda_\text{HC}(L)=\Big\{\bar p=[p_1,\cdots,p_N]:\prod_{n=1}^N \qty(p_n+1)^{\alpha_n} \leq
  \qty(L+1)^{\qty|\vec\alpha|_1} \Big\}.
\end{equation}
As it is desirable to obtain the isotropic case from a reduction of the anisotropic cases, we define the
one-norm for the weights as
\begin{equation}
  |\alpha|_1 = \frac{\sum_{n=1}^N \alpha_n}{N}.
\end{equation}
Considering the same case above ($N=2,L=3$), we apply weights $\alpha_1=5,\alpha_2=3$, and the resulting index
sets are Tables \ref{tab:aniTD} (TD) and \ref{tab:aniHC} (HC).

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c c}
    (2,0) &       &       &       & \\
    (1,0) & (1,1) & (1,2) &       & \\
    (0,0) & (0,1) & (0,2) & (0,3) & (0,4)
  \end{tabular}
  \caption{Anisotropic Total Degree Index Set, $N=2,L=3$}
  \label{tab:aniTD}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c}
    (1,0) &       &       &       \\
    (0,0) & (0,1) & (0,2) & (0,3)
  \end{tabular}
  \caption{Anisotropic Hyperbolic Cross Index Set, $N=2,L=3$}
  \label{tab:aniHC}
\end{table}

There are many methods by which anisotropy weights can be assigned.  Often, if a problem is well-known to an 
analyst, it may be enough to use heuristics to assign importance arbitrarily.  Otherwise, a smaller
uncertainty quantification solve can be used to roughly determine sensitivity coefficients (such as Pearson
coefficients), and the inverse of those can then be applied as anisotropy weights.  Sobol coefficients
obtained from first- or second-order HDMR, discussed later, could also serve as a basis for these weights.
As will be shown, a good choice of anisotropy weight can greatly speed up convergence; however, a
poor choice can slow convergence considerably, as computational resources are used to resolve low-importance
dimensions.


\section{Stochastic Collocation}
Stochastic collocation is the process of using collocated points to approximate integrals of stochastic space
numerically.  In particular we consider using Gaussian quadratures (Legendre, Hermite, Laguerre, and Jacobi)
corresponding to the polynomial expansion polynomials for numerical integration.  Quadrature integration takes
the form
\begin{align}
  \int_a^b f(x)\rho(x) &= \sum_{\ell=1}^\infty w_\ell f(x_\ell),\\
  &\approx \sum_{\ell=1}^L w_\ell f(x_\ell),
\end{align}
where $w_\ell,x_\ell$ are corresponding points and weights belonging to the quadrature set, truncated at order
$\hat L$.  At this point, this $\hat L$ should not be confused with the polynomial expansion truncation order $L$.  We
can simplify this expression using the operator notation
\begin{equation}\label{eq:quad op}
  q^{(\hat L)}[f(x)] \equiv \sum_{\ell=1}^{\hat L} w_\ell f(x_\ell).
\end{equation}
A nominal multidimensional quadrature is the tensor product of
individual quadrature weights and points, and can be written
\begin{align}
  Q^{(\vec{L})} &= q^{(\hat L_1)}_1 \otimes q^{(\hat L_2)}_2 \otimes \cdots,\\
                     &= \bigotimes_{n=1}^N q^{(\hat L_n)}_n.
\end{align}
It is worth noting each quadrature may have distinct points and weights; they need not be constructed using
the same quadrature rule.
In general, one-dimensional Gaussian
quadrature excels in exactly integrating polynomials of order $2p-1$ using $p$ points and weights;
equivalently, it requires $(p+1)/2$ points to integrate an order $p$ polynomial. <TODO> A
summary of the Gaussian quadratures and corresponding probability distribution weight functions are described
in an appendix </TODO>.  For convenience we repeat here the coefficient integral we desire to evaluate, Eq.
\ref{eq:polycoeff}.
\begin{equation}
  u_k = \langle u(Y)\Phi_k(Y) \rangle.
\end{equation}
We can approximate this integral with the appropriate Gaussian quadrature as
\begin{align}
  u_k &\approx Q^{(\vec{\hat L})}[u(Y)\Phi_k(Y)],
\end{align}
where we use bold vector notation to note the order of each individual quadrature,
$\vec{\hat L} = [\hat L_1, \ldots,\hat L_n,\ldots,\hat L_N]$. For clarity, we remove the bold notation and
assume a one-dimensional problem, which extrapolates as expected into the multidimensional case.
\begin{align}
  u_k &\approx q^{(\hat L)}[u(Y)\Phi_k(Y)],\\
      &= \sum_{\ell=1}^{\hat L} w_\ell u(Y_\ell)\Phi_k(Y_\ell).
\end{align}
In order to determine the quadrature order $\hat L$ needed to accurately integrate this expression, we consider the
gPC formulation for $u(Y)$ in Eq. \ref{eq:gPC} and replace it in the sum,
\begin{equation}
  u_k\approx \sum_{\ell=1}^{\hat L} w_\ell \Phi_k(Y_\ell) \sum_{k\in\Lambda(L)}u_{\hat k}\Phi_{\hat k}(Y_\ell).
\end{equation}
Using orthogonal properties of the polynomials, this reduces as $\hat L\to\infty$ to
\begin{equation}
  u_k\approx \sum_{\ell=1}^{\hat L} w_\ell u_k \Phi_k(Y_\ell)^2.
\end{equation}
Thus, the integral, to the same approximation that the gPC expansion itself is approximated, the quadrature is
approximating an integral of order $2k$, and the quadrature order itself should be order 
\begin{equation}
  p=\frac{2k+1}{2}=k+\frac{1}{2}<k+1,
\end{equation}
so we can conservatively use $k+1$ as the quadrature order.  In the case of the largest polynomials with order
$k=L$, the quadrature size $\hat L$ is the same as $L+1$.  It is worth noting that if $u(Y)$ is effectively of
much higher-order polynomial than $L$, this equality for quadrature order does not hold true; however, it also
means that gPC of order $L$ will be a poor approximation.

While a tensor product of highest-necessary quadrature orders could serve as a suitable multidimensional
quadrature set, we can make use of Smolyak-like sparse quadratures to reduce the number of function
evaluations necessary for the TD and HC polynomial index set construction strategies.

\subsection{Smolyak Sparse Grids}
Smolyak sparse grids\cite{smolyak} are an attempt to discover the smallest necessary quadrature set to
integrate a multidimensional integral with varying orders of predetermined quadrature sets.  In our case, the
polynomial index sets determine the quadrature orders each one needs in each dimension to be integrated
accurately.  For example, the polynomial index set point (2,1,3) requires three points in $Y_1$, two in $Y_2$,
and four in $Y_3$,or
\begin{equation}
  Q^{(2,1,3)} = q^{(3)}_1 \otimes q^{(2)}_2 \otimes q^{(4)}_3.
\end{equation}
The full tensor grid of all collocation points would be the tensor product of all quadrature for all points,
or
\begin{equation}
  Q^{(\Lambda(L))} = \bigotimes_{k\in\Lambda}Q^{(k)}.
\end{equation}
Smolyak sparse grids consolidate this tensor form by adding together the points from tensor products of subset
quadrature sets.  Returning momentarily to a one-dimensional problem, we introduce the notation
\begin{equation}
  \Delta_k^{(\hat L)}[f(x)] \equiv \qty(q_k^{(\hat L)} - q_{k-1}^{(\hat L)})[f(x)],
\end{equation}
\begin{equation}
  q_0^{(\hat L)}[f(x)] = 0.
\end{equation}
A Smolyak sparse grid then be defined and applied to the desired integral in Eq. \ref{eq:polycoeff},
\begin{equation}
  S^{(\vec{\hat L})}_{\Lambda,N}[u(Y)\Phi_k(Y)] = \sum_{k\in\Lambda(L)} (\Delta_{k_1}^{(\hat L_1)} \otimes \cdots \otimes
  \Delta_{k_N}^{(\hat L_N)}[u(Y)\Phi_k(Y)].
\end{equation}
Equivalently, and in a more algorithm-friendly approach,
\begin{equation}
  S^{(\vec{\hat L})}_{\Lambda,N}[u(Y)\Phi_k(Y)] = \sum_{k\in\Lambda(L)} c(k)\bigotimes_{n=1}^N
  q^{(\hat L_n)}_n[u(Y)\Phi_k(Y)]
\end{equation}
where
\begin{equation}
  c(k) = \sum_{\substack{j=\{0,1\}^N,\\k+j\in\Lambda}} (-1)^{|j|_1},
\end{equation}
using the traditional 1-norm for $|j|_1$.
The values for $u_k$ can then be calculated as
\begin{align}
  u_k &= \langle u(Y)\Phi_k(Y) \rangle,\\
      &\approx S^{(\vec{\hat L})}_{\Lambda,N}[u(Y)\Phi_k(Y)],
\end{align}
and gPC for $u(Y)$ is complete.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  HDMR                        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{High-Dimension Model Representation (HDMR)}
While using SCgPC is one method for creating a reduced-order model for a simulation code $u(Y)$, another
useful model reduction is HDMR\cite{hdmr}, or Sobol decomposition.  In particular, we consider Cut-HDMR in this work.  
In this methodology, the representation of $u(Y)$ is built up from the contributions of subsets of the input
space.  We consider a reference point realization of $Y$ (nominally the mean of each distribution) $Y^{(0)} =
[Y_1^{(0)},\ldots,Y_N^{(0)}]$ and introduce the notation
\begin{equation}
  \hat Y_n \equiv [Y_1,\ldots,Y_{n-1},Y_{n+1},\ldots,Y_N],
\end{equation}
and
\begin{equation}
  u(Y_n)\hold{\hat Y_n} \equiv u(\hat Y_n^{(0)},Y_n).
\end{equation}
In words, this notation describes holding all the input variables except $Y_n$ constant and only allowing
$Y_n$ to vary.  Similarly for higher dimensions,
\begin{align}
  u(Y_m,Y_n)\hold{\hat Y_{m,n}} &\equiv u(\hat Y_{m,n}^{(0)},Y_m,Y_n),\\
      &=
      u(Y_1^{(0)},\ldots,Y_{m-1}^{(0)},Y_m,Y_{m+1}^{(0)},\ldots,Y_{n-1}^{(0)},Y_n,Y_{n+1}^{(0)},\ldots,Y_N^{(0)}).
\end{align}
The HDMR reduced-order model is then constructed as
\begin{align}
  u(Y) = u_0 &+ \sum_{n=1}^N u_{Y_n} \nonumber\\
  &+ \sum_{n_1=1}^N \sum_{n_2=1}^{n_1-1} u_{Y_{n_1},Y_{n_2}} \nonumber \\
  &+ \sum_{n_1=1}^N \sum_{n_2=1}^{n_1-1} \sum_{n_3=1}^{n_2-1} u_{Y_{n_1},Y_{n_2},Y_{n_3}} \nonumber \\
  &\cdots\\
  &+u_{Y_{n_1},\cdots,Y_{n_N}},
\end{align}
where
\begin{align}
  u_0 &= u(Y^{(0)}),\\
  u_{Y_n} &= u(Y_n)\hold{Y_n} - u_0,\\
  u_{Y_m,Y_n} &= u(Y_m,Y_n)\hold{Y_m,Y_n} - u_{Y_m} - u_{Y_n} - u_0,\\
  \cdots&
\end{align}
If not truncated, this wastes significant computational effort; however, truncating at second- or third-order
interactions can often capture much of the physics with less computation effort than a full solve.  It is
important to note that the HDMR is not a value itself, but a reduced-order model.  To speed up computation
using this model, each component term in the HDMR expansion can be replaced in turn by a reduced-order model.
In particular, because SCgPC is most effective in calculations involving a small input space, gPC
reduced-order models are excellent for representing components of the HDMR expansion.

In addition, Sobol sensitivity indices $s$ can be obtained by taking the ratio of any one expansion term against
the remainder of the terms.  For first-order sensitivities,
\begin{equation}
  s_n = \frac{\text{var}(u_{Y_n})}{\text{var}(u(Y))},
\end{equation}
and for second-order,
\begin{equation}
  s_{m,n} = \frac{\text{var}(u_{Y_m,Y_n})}{\text{var}(u(Y))},
\end{equation}
etc.  These sensitivities can be used to inform adaptive SCgPC calculations on the full model.
